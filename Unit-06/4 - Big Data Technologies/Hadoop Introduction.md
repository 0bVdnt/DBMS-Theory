# Hadoop Introduction (Apache Hadoop)

*   **Definition:** Apache Hadoop is an open-source software framework for storing and processing [[Big Data Introduction|Big Data]] in a [[Distributed Database Systems|distributed environment]] across clusters of [[Commodity Hardware]].
*   **Core Idea:** Inspired by Google papers (GFS, MapReduce). Provides fault-tolerant distributed storage and parallel processing.
*   **Goal:** Enable scalable, reliable, cost-effective analysis of massive datasets, addressing the [[Volume]], [[Velocity]], and [[Variety]] challenges of [[Big Data Introduction|Big Data]]. Relies heavily on [[Scalability (Horizontal vs Vertical)|horizontal scalability]].
*   **Core Components:**
    *   **Storage:** [[HDFS]] (Hadoop Distributed File System)
    *   **Processing (Original):** [[MapReduce]]
    *   **Resource Management (Hadoop 2+):** [[YARN]] (Yet Another Resource Negotiator) - Allows running other frameworks like [[Spark]].
*   **Ecosystem:** Forms the core of a large ecosystem including [[HBase]], [[Hive]], [[Spark]], and many other tools. Often distributed by vendors like [[Cloudera]].

See also: [[Big Data Introduction]], [[HDFS]], [[MapReduce]], [[YARN]], [[HBase]], [[Hive]], [[Spark]], [[Cloudera]], [[Commodity Hardware]], [[Scalability (Horizontal vs Vertical)]]

---
Tags: #hadoop #bigdata #framework #distributed-systems #hdfs #mapreduce #yarn #ecosystem #scalability #fault-tolerance 